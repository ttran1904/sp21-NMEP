{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implent your own Decision Tree/Random Forest!\n",
    "\n",
    "\n",
    "In this python notebook, you will create a basic decision tree on pandas data, and train a classifier on the Iris dataset. Then, you will implement a type of bagging and create a random forest classifier!\n",
    "\n",
    "First, import the required modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then import and preview the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "df = pd.DataFrame(iris.data)\n",
    "df['species'] = iris.target \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have four features labeled 0, 1, 2, and 3. These stand for the length and the width of the sepals and petals, in centimeters. We want to use these four features to predict whether the species is one of three types of Iris plant, labeled 0, 1, or 2. \n",
    "\n",
    "Now, we split the dataset into training and test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['is_train'] = np.random.uniform(0, 1, len(df)) <= .75\n",
    "train, test = df[df['is_train']==True], df[df['is_train']==False]\n",
    "train = train.drop(['is_train'], axis = 1)\n",
    "test = test.drop(['is_train'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disorder (Splitting Metric)\n",
    "\n",
    "First, we want to implement some measure of disorder in a set of data.\n",
    "\n",
    "Implement either information gain or GINI impurity discussed in class. (for reference the equations are in 189 notes here https://www.eecs189.org/static/notes/n25.pdf) \n",
    "\n",
    "\n",
    "The argument `data` is a pandas dataframe containing the features and labels of several data points. We calculate disorder based on the labels, or the last column of the data. Note: make sure that you make this function work for different data (i.e. your function should work for data of different dimensions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def disorder(data):\n",
    "    # Using Gini impurity\n",
    "    if data.empty:\n",
    "        return 1\n",
    "    label_counts = data.iloc[:,-1].value_counts()\n",
    "    classes = label_counts.index\n",
    "    num_rows = data.shape[0]\n",
    "    impurity = 1\n",
    "    for c in classes:\n",
    "        p_k = label_counts[c] / num_rows\n",
    "        impurity -= p_k ** 2\n",
    "    return impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a split function. This function takes in a dataset, and indices for a row and column. We then return two dataframes split on the `column`th feature. The left dataset should contain all of the data where the `column`th feature is greater or equal to the `column`th feature of the `row`th datapoint, and the right should contain the rest. Use the disorder metric you implemented in the function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_on_row_column(data, row, column):\n",
    "    split_val = data.iloc[row, column]\n",
    "    num_rows = data.shape[0]\n",
    "    left = pd.DataFrame()\n",
    "    right = pd.DataFrame()\n",
    "    for i in range(num_rows):\n",
    "        if (data.iloc[i, column] >= split_val):\n",
    "            left = left.append(data.iloc[i])\n",
    "        else:\n",
    "            right = right.append(data.iloc[i])\n",
    "    return left, right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to define our recursive tree class. During training, there are two cases for a node. If the data is all one label, the node is a leaf node, and we return this value during inference. If the data is not all the same label, we find the best split of the data by iterating through all of features and rows in the data. Use the split function defined above to find the best split.\n",
    "\n",
    "Inference takes in a row of a pandas dataframes and returns the predicted class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.feature_index = None\n",
    "        self.split_value = None\n",
    "        self.left_child = None\n",
    "        self.right_child = None\n",
    "        self.best_gini = 1\n",
    "        self.predicted_label = None\n",
    "    \n",
    "    def find_best_split(self):\n",
    "        best_gini = 1\n",
    "        best_feature_index = 0\n",
    "        best_split_value = 0\n",
    "        \n",
    "        num_features = self.data.shape[1] - 1\n",
    "        num_rows = self.data.shape[0]\n",
    "        \n",
    "        for column in range(num_features):\n",
    "            for row in range(num_rows):  \n",
    "                left, right = split_on_row_column(self.data, row, column)\n",
    "                num_rows_left = left.shape[0]\n",
    "                num_rows_right = right.shape[0]\n",
    "                \n",
    "                gini_left = disorder(left)\n",
    "                gini_right = disorder(right)\n",
    "\n",
    "                #weighted average of the Gini impurity of the children\n",
    "                gini = (num_rows_left * gini_left + num_rows_right * gini_right) / num_rows\n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_feature_index = column\n",
    "                    best_split_value = self.data.iloc[row, column]\n",
    "                    best_left = left\n",
    "                    best_right = right\n",
    "\n",
    "        return best_gini, best_feature_index, best_split_value, best_left, best_right\n",
    "    \n",
    "    def train(self):\n",
    "        self.best_gini, self.feature_index, self.split_value, left, right = self.find_best_split()\n",
    "        if disorder(left) == 0:\n",
    "            left_node = Node(left.copy())\n",
    "            left_node.predicted_label = left_node.data.iloc[0, -1]\n",
    "            self.left_child = left_node\n",
    "        else:\n",
    "            left_node = Node(left.copy())\n",
    "            left_node.train()            \n",
    "            self.left_child = left_node\n",
    "        if disorder(right) == 0:\n",
    "            right_node = Node(right.copy())\n",
    "            right_node.predicted_label = right_node.data.iloc[0, -1]\n",
    "            self.right_child = right_node\n",
    "        else:\n",
    "            right_node = Node(right.copy())\n",
    "            right_node.train()\n",
    "            self.right_child = right_node   \n",
    "        return self\n",
    "    \n",
    "    def inference(self, row):\n",
    "        node = self\n",
    "        while node.left_child or node.right_child:\n",
    "            if row[node.feature_index] >= node.split_value:\n",
    "                node = node.left_child\n",
    "            else:\n",
    "                node = node.right_child\n",
    "        return node.predicted_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now initialize and train a decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree = Node(train)\n",
    "tree.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we don't check the training accuracy here (why?). We now want to validate our tree on the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate(model, data):\n",
    "    ct = 0\n",
    "    corr = 0\n",
    "    for i in range(test.shape[0]):\n",
    "        data = test.iloc[i]\n",
    "        ct += 1\n",
    "        if model.inference(data) == data['species']:\n",
    "            corr += 1\n",
    "    return corr/ct\n",
    "\n",
    "validate(tree, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest!\n",
    "\n",
    "Now we will implement data bagging with a random forest! The set up is similar to a single tree. We pass in the data to the forest, along with hyperparameters `n`, `frac`, anbd `m`, which correspond to the number of trees, the fraction of the dataset to use in each bag, the number or percentage of random features (depending on your own implementation) selected at each possible split. Note that the difference between random forests and just bagging  is that random forests select a random subset of features per bag while bagging assumes all features are present in each sample. A good estimate for m in a dataset with `num_features` is m = sqrt(`num_features`). In the inference step we tally the number of votes from each decision tree and return the label with the most amount of votes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Forest:\n",
    "    def __init__(self, data, n, frac):\n",
    "        self.data = data\n",
    "        self.n = n\n",
    "        self.frac = frac\n",
    "        self.trees = []\n",
    "    \n",
    "    def train(self):\n",
    "        for i in range(self.n):\n",
    "            sample = self.data.sample(int(self.frac * self.data.shape[0]))\n",
    "            tree = Node(sample).train()\n",
    "            self.trees.append(tree)\n",
    "        print(self.trees)\n",
    "        \n",
    "    def inference(self, x):\n",
    "        result = []\n",
    "        for i in range(self.n):\n",
    "            result.append(self.trees[i].inference(x))\n",
    "        return max(set(result), key=result.count)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and validate your forest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest = Forest(train, 30, .5)\n",
    "forest.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validate(forest, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
